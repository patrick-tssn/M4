{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import time, random\n",
    "import pysrt\n",
    "from tqdm import tqdm\n",
    "from wtpsplit import SaT\n",
    "# from gpt4api.api_wrap import OpenAIAPIWrapper\n",
    "\n",
    "def fix_youtube_vtt(vtt_file_path) -> str:\n",
    "    \"\"\"Fixes Youtube's autogenerated VTT subtitles and returns a srt-formatted string\"\"\"\n",
    "\n",
    "    import webvtt\n",
    "\n",
    "    pretty_subtitle = ''  \n",
    "    previous_caption_text = ''\n",
    "    i = 1\n",
    "    for caption in webvtt.read(vtt_file_path):\n",
    "\n",
    "        if previous_caption_text == caption.text.strip():\n",
    "            # if previous and current lines are `identical`, print the start time from the previous\n",
    "            # and the end time from the current.\n",
    "            pretty_subtitle += f\"{i}\\n{previous_caption_start} --> {caption.end}\\n{previous_caption_text}\\n\\n\"\n",
    "            i += 1\n",
    "\n",
    "        elif previous_caption_text == caption.text.strip().split(\"\\n\")[0]: \n",
    "            # if the current caption is multiline, and the previous caption is equal to \n",
    "            # the current's first line, just ignore the first line and move on with the second.\n",
    "            previous_caption_text = caption.text.strip().split(\"\\n\")[1]\n",
    "            previous_caption_start = caption.start\n",
    "            last_caption_end = caption.end\n",
    "\n",
    "        else:\t    \n",
    "            previous_caption_text = caption.text.strip()\n",
    "            previous_caption_start = caption.start.strip()\n",
    "\n",
    "    return pretty_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contained(ref, pred):\n",
    "    \"calcuate bleu score\"\n",
    "    ref_lst = ref.lower().split()\n",
    "    pred_lst = pred.lower().split()\n",
    "    h_1 = 0\n",
    "    for ps in pred_lst:\n",
    "        if ps in ref_lst: h_1 += 1\n",
    "    return h_1 / len(pred_lst)\n",
    "        \n",
    "def openai_seg(subtitle_filter):\n",
    "    \"openai for sentence segment\"\n",
    "    prompt = f\"Hi, ChatGPT! I have a series of subtitles from a video broken down into timestamped segments. I would like to merge these subtitles to form complete sentence lists. Here are the subtitles:\\n\\n{subtitle_filter}\\nPlease response with string list, with each element is a complete sentence, don't output any other things.\\nFor example, your response should look like [\"\", ]\"\n",
    "    try_cnt = 5\n",
    "    while try_cnt > 0:\n",
    "        try:\n",
    "            gpt4 = OpenAIAPIWrapper()\n",
    "            subtitle_merge, _ = gpt4.get_completion(prompt)\n",
    "            subtitle_merge = ast.literal_eval(subtitle_merge)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Encounter Error {e}\")\n",
    "            print(f\"Rest retry counts: {try_cnt}\")\n",
    "            try_cnt -= 1\n",
    "            time.sleep(random.randint(3,5))\n",
    "    return subtitle_merge\n",
    "\n",
    "# SaT for sentence segment: https://github.com/segment-any-text/wtpsplit\n",
    "sat = SaT(\"sat-3l-sm\")\n",
    "sat.half().to(\"cuda\")\n",
    "print('load model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess subtitles: sentence segment\n",
    "\n",
    "making sure each dialogue are complete sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 14.88it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subtitles = [\n",
    "    \"--aVuWXcdTs.vtt.en.vtt\",\n",
    "    \"--BL4qjDW8c.vtt.en.vtt\",\n",
    "    \"--SbhVvgxf8.vtt.en.vtt\",\n",
    "    \"-0aYDY57Thc.vtt.en.vtt\",\n",
    "    \"-1syqqeLbHc.vtt.en.vtt\"\n",
    "]\n",
    "\n",
    "for subtitle in tqdm(subtitles):\n",
    "    subtitle_filter = fix_youtube_vtt(f\"videos/{subtitle}\")\n",
    "    with open(f\"videos/{subtitle}_filter.srt\", \"w\") as fp:\n",
    "        fp.write(subtitle_filter)\n",
    "    subtitles = pysrt.open(f\"videos/{subtitle}_filter.srt\")\n",
    "    subtitles_string = [sub.text for sub in subtitles]\n",
    "    subtitles_string = ' '.join(subtitles_string)\n",
    "    # break\n",
    "    # merge sentence\n",
    "    # # way1 openai    \n",
    "    # subtitle_merge = openai_seg(subtitle_filter)\n",
    "    # way2 sat\n",
    "    subtitle_merge = sat.split(subtitles_string, threshold=0.4)\n",
    "    # print(subtitle_merge)\n",
    "    \n",
    "    idx = 0\n",
    "    res = []\n",
    "    prev_sub = \"\"\n",
    "    prev_start = 0.0\n",
    "    prev_end = 0.0\n",
    "    for sub in subtitles:\n",
    "        sub_text = sub.text\n",
    "        h, m, s, ms = sub.start.hours, sub.start.minutes, sub.start.seconds, sub.start.milliseconds\n",
    "        start = h * 3600 + m * 60 + s + ms / 1000\n",
    "        h, m, s, ms = sub.end.hours, sub.end.minutes, sub.end.seconds, sub.end.milliseconds\n",
    "        end = h * 3600 + m * 60 + s + ms / 1000\n",
    "        # if contained(subtitle_merge[idx], sub_text) > 0.6:\n",
    "        if sub_text.lower() in subtitle_merge[idx].lower():\n",
    "            prev_sub += sub_text\n",
    "            prev_end = end\n",
    "        else:\n",
    "            res.append({\n",
    "                \"start\": prev_start,\n",
    "                \"end\": prev_end,\n",
    "                \"subtitle\": subtitle_merge[idx]\n",
    "            })\n",
    "            prev_start = start\n",
    "            prev_end = end\n",
    "            prev_sub = \"\"\n",
    "            idx += 1\n",
    "    if prev_sub:\n",
    "        res.append({\n",
    "            \"start\": prev_start,\n",
    "            \"end\": prev_end,\n",
    "            \"subtitle\": subtitle_merge[idx] \n",
    "        })\n",
    "    with open(\"preprocessed_data/\"+subtitle.split('.')[0]+'.json', \"w\") as jp:\n",
    "        json.dump(res, jp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(video, start, end):\n",
    "    video = video.subclip(start, end)\n",
    "    # get caption for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "from .proxy_task import Tasks\n",
    "\n",
    "dialogues = [\n",
    "    \"--aVuWXcdTs.json\",\n",
    "    \"--BL4qjDW8c.json\",\n",
    "    \"--SbhVvgxf8.json\",\n",
    "    \"-0aYDY57Thc.json\",\n",
    "    \"-1syqqeLbHc.json\"\n",
    "]\n",
    "\n",
    "instructions = []\n",
    "\n",
    "for dias in dialogues:\n",
    "    dias = json.load(open(f\"preprocessed_data/{dias}\"))\n",
    "    \n",
    "    timestamp = random.choice(range(1, len(dias)))\n",
    "    vid = dias.split('.')[0]\n",
    "    vid_path = f\"videos/{vid}.mp4\"\n",
    "    video = VideoFileClip(vid_path)\n",
    "    task = random.choice(Tasks.tasks)\n",
    "    new_vid_path = f\"preprocess/{vid}_{timestamp}.mp4\"\n",
    "    \n",
    "    if task == \"t2d\":\n",
    "        ts = random.choice(list(Tasks.t2d.keys()))\n",
    "        if ts == \"past\":\n",
    "            dialogue = dias[timestamp-1][\"dialogue\"]\n",
    "        elif ts == \"current\":\n",
    "            dialogue = dias[timestamp][\"dialogue\"]\n",
    "        instruct = Tasks.t2d[ts]\n",
    "        answer = dialogue\n",
    "    elif task == 't2c':\n",
    "        ts = random.choice(list(Tasks.t2c.keys()))\n",
    "        if ts == \"past\":\n",
    "            caption = get_caption(video, dias[timestamp-1][\"start\"], dias[timestamp-1][\"end\"])\n",
    "        elif ts == \"current\":\n",
    "            caption = get_caption(video, dias[timestamp][\"start\"], dias[timestamp][\"end\"])\n",
    "        instruct = Tasks.t2d[ts]\n",
    "        answer = caption\n",
    "    elif task == 'd2c':\n",
    "        dialogue = dias[timestamp][\"dialogue\"]\n",
    "        instruct = random.choice(Tasks.d2c).format_map({\"dialogue\": dialogue})\n",
    "        caption = get_caption(video, dias[timestamp][\"start\"], dias[timestamp][\"end\"])\n",
    "        answer = caption\n",
    "    elif task == 'c2d':\n",
    "        caption = get_caption(video, dias[timestamp][\"start\"], dias[timestamp][\"end\"])\n",
    "        instruct = random.choice(Tasks.c2d).format_map({\"caption\": caption})\n",
    "        dialogue = dias[timestamp][\"dialogue\"]\n",
    "        answer = caption\n",
    "    \n",
    "    instructions.append({\n",
    "        \"video\": new_vid_path,\n",
    "        \"instruction\": instruct,\n",
    "        \"answer\": answer,\n",
    "    })\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
